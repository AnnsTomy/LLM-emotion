# -*- coding: utf-8 -*-
"""LLM ASSIGNMENT 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qx1kt6O2g7du087S4Tn540Tc3HQnC5X1
"""

# Install necessary libraries
!pip install datasets transformers nltk wordcloud seaborn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
import nltk
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import torch.nn.functional as F
from keras.preprocessing.sequence import pad_sequences
# Download necessary NLTK data
nltk.download('punkt')

import pandas as pd
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import random

"""EDA

Emotion is a dataset of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise.
"""

# Load the emotion dataset which contains the twitter dataset labelled as emotions.
dataset = load_dataset("dair-ai/emotion","split", trust_remote_code=True)

# Display the dataset structure
print(dataset)

"""Purpose: Convert the dataset splits into Pandas DataFrames for easier manipulation and analysis."""

# Convert to pandas DataFrame
df_train = dataset['train'].to_pandas()
df_valid = dataset['validation'].to_pandas()
df_test = dataset['test'].to_pandas()

# Display basic information
print(df_train.info())

# Display the first few rows of the training set
print(df_train.head())

# Display unique values in the 'label' column
print("Unique labels in training set:")
print(df_train['label'].unique())

# Define the label-to-emotion mapping based on the dataset's documentation
emotion_labels = {
    0: 'sadness',
    1: 'joy',
    2: 'love',
    3: 'anger',
    4: 'fear',
    5: 'surprise'
}

# Map numeric labels to emotion labels
df_train['emotion'] = df_train['label'].map(emotion_labels)

# Display the mapping for verification
print(df_train[['label', 'emotion']].drop_duplicates())

# Check for null values in the DataFrame
null_values = df_train.isnull().sum()

# Display columns with null values and their counts
print("Null values in each column:\n", null_values)

# Optionally, display the rows with null values
print("\nRows with null values:\n", df_train[df_train.isnull().any(axis=1)])

# Handle null values (if any) - here we drop rows with null values
df_train = df_train.dropna()

# Verify that there are no more null values
print("\nNull values after handling:\n", df_train.isnull().sum())

# Method to convert numeric labels to string emotions
def int2str(label):
    return emotion_labels[label]

# Apply the method to the DataFrame
df_train['emotion'] = df_train['label'].apply(int2str)

# Display the mapping for verification
print(df_train[['label', 'emotion']].drop_duplicates())

# Display a few rows to verify the mapping
print(df_train[['text', 'label', 'emotion']].head())

import plotly.express as px

px.bar(df_train['label'].value_counts(ascending=True),template='plotly_dark')

# Plot the distribution of emotion labels
plt.figure(figsize=(10, 6))
sns.countplot(data=df_train, x='emotion', order=df_train['emotion'].value_counts().index)
plt.title('Distribution of Emotion Labels in the Training Set')
plt.xlabel('Emotion')
plt.ylabel('Count')
plt.show()

"""So here the most common is joy and least sentences are of surprise

Analyze if text length varies across different emotions using a boxplot.
"""

# Calculate text lengths
df_train['text_length'] = df_train['text'].apply(len)

# Plot text length distribution by emotion
plt.figure(figsize=(12, 8))
sns.boxplot(data=df_train, x='emotion', y='text_length', order=df_train['emotion'].value_counts().index)
plt.title('Text Length Distribution by Emotion')
plt.xlabel('Emotion')
plt.ylabel('Text Length')
plt.show()

"""Text length doesnt influence the emotion"""

# Display sample texts for each emotion
for emotion in df_train['emotion'].unique():
    print(f"\nSamples for emotion: {emotion}")
    print(df_train[df_train['emotion'] == emotion]['text'].head(), "\n")

from wordcloud import WordCloud

# Combine all text into a single string
text = ' '.join(df_train['text'].tolist())

# Generate a word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

# Plot the word cloud
plt.figure(figsize=(12, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud for Training Set')
plt.show()

"""most common words are feel, feeling, know, will, think, make, really, thing , way etc.

Verify unique emotions and their counts in the training set.
"""

# Display the unique emotions in the training set
unique_emotions = df_train['emotion'].unique()
print("\nUnique emotions in the training set:")
print(unique_emotions)

# Display the distribution of emotions in the training set
emotion_counts = df_train['emotion'].value_counts()
print("\nDistribution of emotions in the training set:")
print(emotion_counts)

# Display sample texts for each emotion
for emotion in unique_emotions:
    print(f"\nSamples for emotion: {emotion}")
    print(df_train[df_train['emotion'] == emotion]['text'].head(), "\n")

# Calculate the number of words in each tweet
df_train['word_count'] = df_train['text'].apply(lambda x: len(x.split()))

# Plot the distribution of word counts
plt.figure(figsize=(10, 6))
sns.histplot(df_train['word_count'], bins=30, kde=True)
plt.title('Distribution of Word Counts in Tweets')
plt.xlabel('Word Count')
plt.ylabel('Frequency')
plt.show()

# Reduce the size of the dataset
n_samples = 100  # Specify the number of samples you want to use
df_train = df_train.sample(n=n_samples, random_state=42)

"""TOKENIZATION"""

!pip install nltk
# Install necessary libraries
!pip install datasets nltk wordcloud seaborn
import nltk
from nltk.tokenize import word_tokenize
# Download necessary NLTK data
nltk.download('punkt')

# Tokenize the text data
df_train['tokens'] = df_train['text'].apply(word_tokenize)

# Display a few rows to verify the tokenization
print(df_train[['text', 'tokens', 'emotion']].head())

# Calculate the number of tokens in each tweet (using the previously tokenized text)
df_train['token_count'] = df_train['tokens'].apply(len)

# Plot the distribution of token counts
plt.figure(figsize=(10, 6))
sns.histplot(df_train['token_count'], bins=30, kde=True)
plt.title('Distribution of Token Counts in Tweets')
plt.xlabel('Token Count')
plt.ylabel('Frequency')
plt.show()

# Character Tokenization
df_train['char_tokens'] = df_train['text'].apply(list)

# Display a few rows to verify the tokenization
print(df_train[['text', 'char_tokens']].head())

""" Tokenize the text data to prepare for further text analysis tasks."""

# Create a set of all unique characters in the text data
unique_chars = set(''.join(df_train['text'].tolist()))

# Display the unique characters
print(f"Unique characters: {unique_chars}")

# Create a mapping from each character to a unique integer
token2idx = {char: idx for idx, char in enumerate(unique_chars)}

# Display the token2idx mapping
print(f"Character to Integer Mapping (token2idx): {token2idx}")

# Convert each character in the text to its corresponding integer
df_train['char_ids'] = df_train['text'].apply(lambda x: [token2idx[char] for char in x])

# Display a few rows to verify the conversion
print(df_train[['text', 'char_ids']].head())

# Create a reverse mapping from integer to character
idx2token = {idx: char for char, idx in token2idx.items()}

# Display the reverse mapping
print(f"Integer to Character Mapping (idx2token): {idx2token}")

# Create a set of all unique words in the text data
unique_words = set(word for tokens in df_train['tokens'] for word in tokens)

# Create a mapping from each word to a unique integer
word2idx = {word: idx for idx, word in enumerate(unique_words)}

from torch.utils.data import DataLoader, TensorDataset
# Let's represent text in numerical format
input_ids = []
for tokens in df_train['tokens']:
    input_ids.append([word2idx[token] for token in tokens])

print(f'{len(input_ids)} tokenized sentences')
print(input_ids[:5])  # Print the first 5 tokenized sentences

# Pad sequences to the same length
input_ids_padded = pad_sequences(input_ids, padding='post')

# Convert the list of lists into a tensor of integers
inputs_ids = torch.tensor(input_ids_padded, dtype=torch.long)

# Create a dataset from inputs_ids
dataset = TensorDataset(inputs_ids)

# Create a DataLoader to handle batching
batch_size = 8
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Function to process batches
def process_batch(batch):
    batch = batch[0]  # Extract the input_ids from the tuple
    one_hot_encodings = F.one_hot(batch, num_classes=len(word2idx))
    return one_hot_encodings

# Process and print one batch to check
for batch in dataloader:
    one_hot_batch = process_batch(batch)
    print(f'OHE batch size: {one_hot_batch.shape}')
    break  # Just process one batch to check

# If you need to process all batches, you can loop through the dataloader
for batch in dataloader:
    one_hot_batch = process_batch(batch)
    # Now you can use one_hot_batch for training or further processing
    # Ensure to move it to the appropriate device if using a GPU:
    # one_hot_batch = one_hot_batch.to(device)
    # For now, we just print the shape
    print(f'OHE batch size: {one_hot_batch.shape}')

from sklearn.preprocessing import MinMaxScaler
# Initialize the MinMaxScaler
scaler = MinMaxScaler()

# Function to process batches
def process_batch(batch):
    batch = batch[0]  # Extract the input_ids from the tuple
    one_hot_encodings = F.one_hot(batch, num_classes=len(word2idx)).float()

    # Reshape the batch to fit the scaler
    batch_shape = one_hot_encodings.shape
    one_hot_encodings_flat = one_hot_encodings.view(-1, one_hot_encodings.shape[-1])

    # Apply MinMax scaling
    one_hot_encodings_scaled = scaler.fit_transform(one_hot_encodings_flat)

    # Reshape back to the original batch shape
    one_hot_encodings_scaled = torch.tensor(one_hot_encodings_scaled).view(batch_shape)

    return one_hot_encodings_scaled

# Process and print one batch to check
for batch in dataloader:
    scaled_batch = process_batch(batch)
    print(f'Scaled OHE batch size: {scaled_batch.shape}')
    break  # Just process one batch to check

# Process all batches and collect scaled batches
scaled_batches = []
for batch in dataloader:
    scaled_batch = process_batch(batch)
    scaled_batches.append(scaled_batch)
    # Ensure to move it to the appropriate device if using a GPU:
    # scaled_batch = scaled_batch.to(device)
    # For now, we just print the shape
    print(f'Scaled OHE batch size: {scaled_batch.shape}')

# If you need to concatenate all scaled batches into a single tensor
scaled_data = torch.cat(scaled_batches)

print(f'Final scaled data size: {scaled_data.shape}')