# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JUvvTjpMHVe65DRb6GX5hHGQ6twpGAMX
"""

import pandas as pd
import numpy as np
import warnings; warnings.filterwarnings('ignore')

# Install and load necessary libraries
!pip install datasets
from datasets import load_dataset
from datasets import Dataset, DatasetDict, Features, Value, ClassLabel
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, Trainer, TrainingArguments
import torch
from sklearn.preprocessing import MinMaxScaler
from sklearn.manifold import TSNE
from sklearn.dummy import DummyClassifier
from sklearn.linear_model import LogisticRegression as LR
from sklearn.metrics import accuracy_score, f1_score
import plotly.express as px
import os

# Load the emotion dataset
emotion_dataset = load_dataset("dair-ai/emotion", "split", trust_remote_code=True)
print(emotion_dataset)

# Splitting the dataset into train, validation, and test sets
train_dataset = emotion_dataset['train']
validation_dataset = emotion_dataset['validation']
test_dataset = emotion_dataset['test']

# Displaying the first few examples from each split to confirm the splits
print("Train Dataset: ", train_dataset[:3])
print("Validation Dataset: ", validation_dataset[:3])
print("Test Dataset: ", test_dataset[:3])

# Convert the datasets to pandas DataFrames
train_df = train_dataset.to_pandas()
validation_df = validation_dataset.to_pandas()
test_df = test_dataset.to_pandas()

# Display the first few rows of each DataFrame to confirm the conversion
print("Train DataFrame:\n", train_df.head())
print("\nValidation DataFrame:\n", validation_df.head())
print("\nTest DataFrame:\n", test_df.head())

# Extract labels from each DataFrame
train_labels = train_df['label']
validation_labels = validation_df['label']
test_labels = test_df['label']

# Display the first few labels to confirm
print("Train Labels:\n", train_labels.head())
print("\nValidation Labels:\n", validation_labels.head())
print("\nTest Labels:\n", test_labels.head())

# Define the label names
label_names = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']

# Create a mapping from numeric labels to label names
label_mapping = {i: label for i, label in enumerate(label_names)}

# Map the numeric labels to label names in each DataFrame
train_df['label_name'] = train_df['label'].map(label_mapping)
validation_df['label_name'] = validation_df['label'].map(label_mapping)
test_df['label_name'] = test_df['label'].map(label_mapping)

# Display the first few rows of each DataFrame to confirm the mapping
print("Train DataFrame with Label Names:\n", train_df.head())
print("\nValidation DataFrame with Label Names:\n", validation_df.head())
print("\nTest DataFrame with Label Names:\n", test_df.head())

# Check for missing values in the train DataFrame
train_missing = train_df.isnull().sum()
print("Missing values in Train DataFrame:\n", train_missing)

# Check for missing values in the validation DataFrame
validation_missing = validation_df.isnull().sum()
print("\nMissing values in Validation DataFrame:\n", validation_missing)

# Check for missing values in the test DataFrame
test_missing = test_df.isnull().sum()
print("\nMissing values in Test DataFrame:\n", test_missing)

# Check for duplicates in the train DataFrame
train_duplicates = train_df.duplicated().sum()
print("Number of duplicate rows in Train DataFrame:", train_duplicates)

# Check for duplicates in the validation DataFrame
validation_duplicates = validation_df.duplicated().sum()
print("Number of duplicate rows in Validation DataFrame:", validation_duplicates)

# Check for duplicates in the test DataFrame
test_duplicates = test_df.duplicated().sum()
print("Number of duplicate rows in Test DataFrame:", test_duplicates)

# Remove the duplicate row from the Train DataFrame
train_df = train_df.drop_duplicates()

# Verify that the duplicate has been removed
train_duplicates_after = train_df.duplicated().sum()
print("Number of duplicate rows in Train DataFrame after removal:", train_duplicates_after)

import matplotlib.pyplot as plt
import seaborn as sns

# Set up the plotting style
sns.set(style="whitegrid")

# Plot the distribution of labels in the Train DataFrame
plt.figure(figsize=(10, 6))
sns.countplot(x='label_name', data=train_df, order=train_df['label_name'].value_counts().index, palette='viridis')
plt.title('Distribution of Labels in the Train Dataset')
plt.xlabel('Emotion')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

# Calculate the length of each text sample in the Train DataFrame
train_df['text_length'] = train_df['text'].apply(len)

# Plot the distribution of text lengths
plt.figure(figsize=(12, 6))
sns.histplot(train_df['text_length'], bins=30, kde=True, color='blue')
plt.title('Distribution of Text Lengths in the Train Dataset')
plt.xlabel('Text Length')
plt.ylabel('Frequency')
plt.show()

# Boxplot to see the text length distribution by label
plt.figure(figsize=(12, 6))
sns.boxplot(x='label_name', y='text_length', data=train_df, palette='viridis')
plt.title('Text Length Distribution by Label in the Train Dataset')
plt.xlabel('Emotion')
plt.ylabel('Text Length')
plt.xticks(rotation=45)
plt.show()

# Pairplot of text length by emotion
sns.pairplot(train_df, hue='label_name', vars=['text_length'], palette='viridis')
plt.show()

